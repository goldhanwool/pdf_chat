
Scaled Dot-Product Attention Multi-Head Attention
스케일된 도트-제품 주의 멀티 헤드 주의

 | Vv
| Vv

 Figure 2: (left) Scaled Dot-Product Attention.
그림 2: (왼쪽) Scale Dot-Product 주의점.

(right) Multi-Head Attention consists of several attention layers running in parallel.
(오른쪽) 멀티 헤드 어텐션은 병렬로 실행되는 여러 어텐션 레이어로 구성됩니다.

 of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.
값들 중에서, 각각의 값에 할당된 가중치는 해당 키와의 쿼리의 호환성 함수에 의해 계산된다.

 3.2.1 Scaled Dot-Product Attention
3.2.1 스케일 도트-제품 주의

 We call our particular attention "Scaled Dot-Product Attention" (Figure 2).
우리는 우리의 특별한 관심을 "스케일드 도트-프로덕트 어텐션"이라고 부릅니다(그림 2).

The input consists of queries and keys of dimension d,, and values of dimension d,.
입력은 차원 d의 쿼리 및 키, 차원 d의 값으로 구성된다.

We compute the dot products of the query with all keys, divide each by /d,, and apply a softmax function to obtain the weights on the values.
우리는 모든 키로 쿼리의 도트 곱을 계산하고 각각을 /d로 나누고 소프트맥스 함수를 적용하여 값에 대한 가중치를 얻는다.

 In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
실제로, 우리는 행렬 Q로 함께 포장된 일련의 쿼리에 대한 주의 함수를 동시에 계산한다.

The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:
키들과 값들은 행렬 K와 V에 함께 포장된다. 우리는 출력 행렬을 다음과 같이 계산한다:

 QKT vay
QKT Vay

 The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention.
가장 일반적으로 사용되는 두 가지 주의 함수는 가산 주의[2]와 점 곱 주의(다중 곱하기)이다.

Dot-product attention is identical to our algorithm, except for the scaling factor of Tm: Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.
점 제품 주의는 Tm의 스케일링 인자를 제외하고는 우리의 알고리즘과 동일하다: 가산 주의는 단일 은닉 계층을 가진 피드 포워드 네트워크를 사용하여 호환성 함수를 계산한다.

While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.
이 둘은 이론적 복잡성이 유사하지만, 도트 곱 주의는 매우 최적화된 행렬 곱 코드를 사용하여 구현할 수 있기 때문에 실제로 훨씬 빠르고 공간 효율적이다.

 Attention(Q, K, V) = softmax
주의사항(Q, K, V) = softmax

 a a)
a)

 While for small values of dj, the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d; [3].
dj의 작은 값의 경우, 두 메커니즘이 유사하게 수행되지만, d의 더 큰 값에 대해 스케일링 없이 가산 주의가 도트 제품 주의를 능가한다; [3].

We suspect that for large values of dj, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients *.
우리는 dj의 큰 값의 경우, 도트 곱의 크기가 크게 증가하여 소프트맥스 함수를 극도로 작은 그래디언트 *를 갖는 영역으로 밀어 넣는 것으로 의심한다.

To counteract this effect, we scale the dot products by vr
이 효과를 상쇄하기 위해 점 제품을 vr로 스케일링한다

 3.2.2 Multi-Head Attention
3.2.2 다중 머리 주의

 Instead of performing a single attention function with dmodei-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dj., dx and d, dimensions, respectively.
디모데 차원 키, 값 및 쿼리로 단일 주의 기능을 수행하는 대신, 쿼리, 키 및 값을 서로 다른 학습된 선형 투영으로 h번 선형 투영하여 각각 dj, dx 및 d 차원으로 선형 투영하는 것이 유익하다는 것을 발견했다.

On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d,-dimensional
이렇게 예측된 각 버전의 쿼리, 키 및 값에서 주의 함수를 병렬로 수행하여 d,차원을 산출합니다

 “To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1.
"점 제품이 왜 커지는지 설명하기 위해 q와 k의 성분은 평균이 0이고 분산이 1인 독립적인 확률 변수라고 가정하자.

Then their dot product, q- k = 7“, qikz, has mean 0 and variance di.
그러면 그들의 내적인 q-k = 7", qikz는 평균 0과 분산 di를 갖는다.
